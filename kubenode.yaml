heat_template_version: 2013-05-23

description: >
  This is a nested stack that defines a single Kubernetes minion,
  based on a vanilla Fedora 20 cloud image.  This stack is included by
  a ResourceGroup resource in the parent template (kubecluster.yaml).

parameters:

  server_image:
    type: string
    default: fedora-20-x86_64-updated
    description: glance image used to boot the server

  server_flavor:
    type: string
    default: m1.small
    description: flavor to use when booting the server

  ssh_key_name:
    type: string
    description: name of ssh key to be provisioned on our server
    default: lars

  external_network_id:
    type: string
    description: uuid of a network to use for floating ip addresses

  bridge_address_base:
    type: string
    description: >
      first two octets of a /16 network to use for minion
      addresses.
    default: 10.251

  linkmanager_key:
    type: string
    description: >
      used to sign etcd keys that control vxlan
      overlay network.

  docker_volume_size:
    type: number
    description: size of docker volume
    default: 20

  gluster_volume_size:
    type: number
    description: size of volume allocated for gluster storage
    default: 40

  # The following are all generated in the parent template.
  kube_master_ip:
    type: string
    description: IP address of the Kubernetes master server.
  fixed_network_id:
    type: string
    description: Network from which to allocate fixed addresses.
  fixed_subnet_id:
    type: string
    description: Subnet from which to allocate fixed addresses.

resources:

  node_wait_handle:
    type: "AWS::CloudFormation::WaitConditionHandle"

  node_wait_condition:
    type: "AWS::CloudFormation::WaitCondition"
    depends_on:
      - kube_node
    properties:
      Handle:
        get_resource: node_wait_handle
      Timeout: "6000"

  # I am lazy, so this opens up all ports.
  secgroup_all_open:
    type: "OS::Neutron::SecurityGroup"
    properties:
      rules:
        - protocol: icmp
        - protocol: tcp
        - protocol: udp

  kube_node:
    type: "OS::Nova::Server"
    properties:
      image:
        get_param: server_image
      flavor:
        get_param: server_flavor
      key_name:
        get_param: ssh_key_name
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/sh

            bridge_address_base="$BRIDGE_ADDRESS_BASE"

            yum -y remove NetworkManager
            chkconfig network on

            # enable dnf command
            yum -y install dnf-plugins-core

            # enable kubernetes repository
            dnf -y copr enable walters/atomic-next
            sed -i 's/$releasever/21/g' /etc/yum.repos.d/_copr_walters-atomic-next.repo

            # avoid conflicts with "docker" package in fedora 20 that is not
            # the docker you are looking for.
            dnf -y copr enable larsks/fakedocker

            yum -y upgrade
            yum -y install \
              jq openvswitch bridge-utils docker-io \
              git python-netifaces python-requests \
              tcpdump python-netifaces python-setuptools \
              golang-github-docker-libcontainer kubernetes \
              glusterfs-server autofs

            # this is required to implement "exec" type livenessProbes in
            # kubernetes.
            ln -s /usr/bin/nsinit /usr/sbin/nsinit

            myip=$(ip addr show eth0 | awk '$1 == "inet" {print $2}' | cut -f1 -d/)
            myip_last_octet=${myip##*.}
            bridge_address="${bridge_address_base}.${myip_last_octet}.1"
            netconf=/etc/sysconfig/network-scripts

            # Docker contains are attached to this bridge
            cat > $netconf/ifcfg-kbr0 <<EOF
            DEVICE=kbr0
            TYPE=Bridge
            IPADDR=${bridge_address}
            NETMASK=255.255.255.0
            ONBOOT=yes
            STP=yes

            # With the default forwarding delay of 15 seconds,
            # many operations in a 'docker build' will simply timeout
            # before the bridge starts forwarding.
            DELAY=2
            EOF

            # This bridge will handle VXLAN tunnels
            cat > $netconf/ifcfg-obr0 <<EOF
            DEVICE=obr0
            DEVICETYPE=ovs
            TYPE=OVSBridge
            ONBOOT=yes
            BRIDGE=kbr0
            STP=true
            EOF

            cat > $netconf/route-kbr0 <<EOF
            ${bridge_address_base}.0.0/16 dev kbr0 scope link src ${bridge_address}
            EOF

            # Container interface MTU must be reduced in order to
            # prevent fragmentation problems when vxlan header is
            # added to a host-MTU sized packet.
            cat > /etc/sysconfig/docker <<EOF
            OPTIONS="--selinux-enabled -b kbr0 --mtu 1450"
            EOF

            sed -i '/^KUBE_ETCD_SERVERS=/ s|=.*|=http://$KUBE_MASTER_IP:4001|' \
              /etc/kubernetes/config
            sed -i '
              /^MINION_ADDRESS=/ s/=.*/="0.0.0.0"/
              /^MINION_HOSTNAME=/ s/=.*/="'"$myip"'"/
            ' /etc/kubernetes/kubelet

            sed -i '
              /^KUBE_API_ADDRESS=/ s/=.*/="$KUBE_MASTER_IP"/
              /^KUBE_MASTER=/ s/=.*/="$KUBE_MASTER_IP"/
            ' /etc/kubernetes/apiserver

            # install linkmanager for managing OVS links
            # for minion overlay network.
            git clone http://github.com/larsks/linkmanager.git \
              /opt/linkmanager
            (
              cd /opt/linkmanager
              python setup.py install
              cp linkmanager.service /etc/systemd/system/linkmanager.service
            )
            cat > /etc/sysconfig/linkmanager <<EOF
            OPTIONS="-s http://$KUBE_MASTER_IP:4001 -v -b obr0 --secret $LINKMANAGER_KEY"
            EOF

            cat >> /etc/environment <<EOF
            KUBERNETES_MASTER=http://$KUBE_MASTER_IP:8080
            EOF

            # start bridges first
            systemctl enable openvswitch
            systemctl start openvswitch
            ifup kbr0
            ifup obr0

            # sometimes symlinks in /dev/disk/by-id are missing.  see if this helps.
            udevadm trigger

            # we need to truncate the volume id, because apparently the system
            # truncates device serial numbers to 20 characters.
            docker_volume="$DOCKER_VOLUME"
            docker_volume=${docker_volume::20}
            mke2fs -m0 -L docker /dev/disk/by-id/virtio-$docker_volume

            gluster_volume="$GLUSTER_VOLUME"
            gluster_volume=${gluster_volume::20}
            mke2fs -m0 -L gluster /dev/disk/by-id/virtio-$gluster_volume

            echo "$DOCKER_VOLUME -> $docker_volume"
            echo "$GLUSTER_VOLUME -> $gluster_volume"
            ls -l /dev/disk/by-id/

            mkdir -p /var/lib/docker /bricks
            cat >> /etc/fstab <<EOF
            /dev/disk/by-id/virtio-$docker_volume /var/lib/docker ext4 defaults 1 2
            /dev/disk/by-id/virtio-$gluster_volume /bricks ext4 defaults 1 2
            EOF

            mount /var/lib/docker
            mount /bricks

            mkdir /gluster
            cat > /etc/auto.gluster <<'EOF'
            #!/bin/sh
            opts="-fstype=glusterfs,nodev,nosuid"
            echo "$opts 127.0.0.1:/$1"
            EOF
            chmod 755 /etc/auto.gluster

            cat > /etc/auto.master.d/gluster.autofs <<EOF
            /gluster /etc/auto.gluster
            EOF

            setenforce 0
            sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config

            # then other services
            for service in docker.socket kubelet kube-proxy linkmanager glusterd autofs; do
              systemctl enable $service
              systemctl start $service
            done

            # make fedora user a member of the docker group
            # (so you can run docker commands as the fedora user)
            usermod -G docker fedora

            cfn-signal -e0 --data 'OK' -r 'Setup complete' '$WAIT_HANDLE'
          params:
            "$KUBE_MASTER_IP":
              get_param: kube_master_ip
            "$BRIDGE_ADDRESS_BASE":
              get_param: bridge_address_base
            "$LINKMANAGER_KEY":
              get_param: linkmanager_key
            "$WAIT_HANDLE":
              get_resource: node_wait_handle
            "$DOCKER_VOLUME":
              get_resource: docker_volume
            "$GLUSTER_VOLUME":
              get_resource: gluster_volume
      networks:
        - port:
            get_resource: kube_node_eth0

  kube_node_eth0:
    type: "OS::Neutron::Port"
    properties:
      network_id:
        get_param: fixed_network_id
      security_groups:
        - get_resource: secgroup_all_open
      fixed_ips:
        - subnet_id:
            get_param: fixed_subnet_id

  kube_node_floating:
    type: "OS::Neutron::FloatingIP"
    properties:
      floating_network_id:
        get_param: external_network_id
      port_id:
        get_resource: kube_node_eth0

  docker_volume:
    type: "OS::Cinder::Volume"
    properties:
      size: {get_param: docker_volume_size}

  docker_volume_attach:
    type: "OS::Cinder::VolumeAttachment"
    properties:
      instance_uuid: {get_resource: kube_node}
      volume_id: {get_resource: docker_volume}

  gluster_volume:
    type: "OS::Cinder::Volume"
    properties:
      size: {get_param: gluster_volume_size}

  gluster_volume_attach:
    type: "OS::Cinder::VolumeAttachment"
    properties:
      instance_uuid: {get_resource: kube_node}
      volume_id: {get_resource: gluster_volume}

outputs:

  kube_node_ip:
    value: {get_attr: [kube_node_eth0, fixed_ips, 0, ip_address]}
  
  kube_node_external_ip:
    value: {get_attr: [kube_node_floating, floating_ip_address]}

